{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <B> Deploy models using the framework (Torch) </B>\n",
    "* Contatiner: conda_pytorch_p39"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference scratch\n",
    "\n",
    "### 본 워크샵의 모든 노트북은 `conda_pytorch_p39` 여기에서 작업 합니다.\n",
    "\n",
    "이 노트북은 아래와 같은 작업을 합니다.\n",
    "\n",
    "- 0. 개념 확인\n",
    "- 1. 환경 셋업\n",
    "- 2. 모델 아티펙트 다운로드 및 압축해제\n",
    "- 3. 추론 함수 로컬 테스트\n",
    "- 4. 로컬 엔드포인트 생성\n",
    "- 5. 로컬 추론\n",
    "- 6. 로컬 엔드 포인트 삭제\n",
    "\n",
    "\n",
    "### 참고: \n",
    "- 세이지 메이커 개발자 가이드 --> [추론을 위한 모델 배포](https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/deploy-model.html)\n",
    "- 세이지 메이커 배포에 대한 웹비나 --> [Amazon SageMaker 기반 사전 훈련된 딥러닝 모델 손쉽게 배포하기 – 김대근:: AWS Innovate 2021](https://www.youtube.com/watch?v=ZdOcrLKow3I)\n",
    "- 세이지 메이커 호스팅 기본 컨셉 --> [SageMaker 호스팅 아키텍쳐](https://github.com/gonsoomoon-ml/SageMaker-Pipelines-Step-By-Step/blob/main/scratch/8.1.Deploy-Pipeline.ipynb)\n",
    "- 세이지 메이커로 파이토치 사용 --> [Use PyTorch with the SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html)\n",
    "    \n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Concept check\n",
    "![nn](images/inference_container.png)\n",
    "\n",
    "**[소스]: 위에 \"참조\" 에 언급된 \"세이지 메이커 배포에 대한 웹비나\" 의 내용입니다.**\n",
    "\n",
    "AWS가 관리하는 배포 컨테이너에 대해 좀 더 자세히 살펴 보겠습니다. 각 프레임워크별에 적합한 배포 컨테이너들이 사전 빌드되어 있으며, 텐서플로는 텐서플로 서빙, 파이토치는 torchserve, MXNet은 MMS, scikit learn은 Flask가 내장되어 있습니다. 이 중에서 파이토치의 경우, 기존에는 MMS가 내장되어 있었지만, 2020년 말부터 Amazon과 facebook이 공동으로 개발한 torchserve를 내장하기 시작했습니다. \n",
    "\n",
    "배포 컨테이너를 구동할 때에는 추론을 위한 http 요청을 받아들일 수 있는 RESTful API를 실행하는 serve 명령어가 자동으로 실행되면서 엔드포인트가 시작됩니다. 엔드포인트를 시작할 때, SageMaker는 도커 컨테이너에서 사용 가능한 외부의 모델 아티팩트, 데이터, 그리고 기타 환경 설정 정보 등을 배포 인스턴스의 /opt/ml 폴더로 로딩합니다. \n",
    "/opt/ml 폴더는 도커 컨테이너의 밖에서 접근 가능한 머신 러닝 관련 파일들의 볼륨을 의미하며, 이 경로에 대한 내용을 알고 있으면 디버깅에 도움이 됩니다.\n",
    "\n",
    "물론, 훈련과 호스팅을 모두 지원하는 단일 도커 이미지 빌드도 가능하기 때문에 오픈 소스 도커파일을 참조해서 여러분만의 환경을 구성하는 것도 가능합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model_artifact_structure.png](images/model_artifact_structure.png)\n",
    "\n",
    "**[소스]: 위에 \"참조\" 에 언급된 \"세이지 메이커 배포에 대한 웹비나\" 의 내용입니다.**\n",
    "\n",
    "우선 TensorFlow의 경우는 model 경로를 별도로 생성해 줘야 합니다. Model의 하위 폴더에는 모델 버전 번호 폴더를 생성한 다음 그 안에 훈련된 모델 파일을 넣으시면 됩니다. 이 때 주의하실 점이 하나 있는데, pb 파일은 SavedModel만 사용하셔야 하며, Frozen Graph 파일은 SavedModel로 변환해서 사용하셔야 합니다.\n",
    "PyTorch, MXNet은 디렉토리 및 파일 구조가 유사합니다. 별도의 폴더를 생성하지 않고 곧바로 모델 파라메터 파일을 넣으시면 됩니다. \n",
    "\n",
    "그리고 code 폴더는 추론에 필요한 스크립트 파일과 requirements.txt를 포함하면 되는데, PyTorch와 MXNet은 굳이 이 폴더를 포함하지 않아도 됩니다. <br>\n",
    "\n",
    "- inference.py, requirements.txt는 sagemaker.pytorch.model.PyTorchModel의 클래스 생성자에 entry_point, source_dir을 통해서 제공이 됩니다. 이와 같의 스크립트 모드 방식으로 제공이 가능합니다.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![inference_handler.png](images/inference_handler.png)\n",
    "\n",
    "**[소스]: 위에 \"참조\" 에 언급된 \"세이지 메이커 배포에 대한 웹비나\" 의 내용입니다.**\n",
    "\n",
    "이제, 추론에 필요한 Python 스크립트 인터페이스를 살펴 보겠습니다. 이 인터페이스들은 SageMaker Inference Toolkit이 정의한 인터페이스로, 텐서플로를 제외한 프레임워크들에서 공용으로 사용됩니다. \n",
    "\n",
    "model_fn() 함수는 S3나 model zoo에 저장된 모델을 추론 인스턴스의 메모리로 로드 후, 모델을 리턴하는 방법을 정의하는 전처리 함수입니다.\n",
    "\n",
    "input_fn() 함수는 사용자로부터 입력받은 내용을 모델 추론에 적합하게 변환하는 전처리 함수로, content_type 인자값을 통해 입력값 포맷을 확인할 수 있습니다.\n",
    "\n",
    "predict_fn() 함수는 추론 함수로, model_fn()에서 리턴받은 모델과 input_fn()에서 변환된 데이터로 추론을 수행합니다.\n",
    "\n",
    "output_fn() 함수는 추론 결과를 반환하는 후처리 함수입니다.\n",
    "\n",
    "그리고 input_fn(), predict_fn(), output_fn()을 각각 구현하는 대신, 세 함수들을 한꺼번에 묶어서 transform() 함수에 구현하는 것도 가능합니다. \n",
    "처음에는 익숙치 않을 수 있지만 AWS 공식 예제들이나 레퍼런스의 제 핸즈온을 한 번 해 보시면 금방 익숙해질 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoReload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. parameter store 셋팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from utils.ssm import parameter_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "strRegionName=boto3.Session().region_name\n",
    "pm = parameter_store(strRegionName)\n",
    "strPrefix = pm.get_params(key=\"PREFIX\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Check functions in inference.py with local env.\n",
    "[중요] inference.py를 만들어 주어야 함\n",
    "* model_fn: 학습한 모델 로드\n",
    "* input_fn: endpoint invocation시 전달 되는 input 처리 하는 함수\n",
    "* predict_fn: forword propagation, input_fn의 이후 호출 \n",
    "* output_fn: 유저에게 결과 전달\n",
    "\n",
    "- 사용자 정의 inference 코드를 정의해서 사용하기 전에, 노트북에서 사전 테스트 및 디버깅을 하고 진행하면 빠르게 추론 개발을 할수 있습니다.\n",
    "- 디폴트 inference code (input_fn, model_fn, predict_fn, output_fn) 을 사용해도 되지만, 상황에 따라서는 사용자 정의가 필요할 수 있습니다. 디폴트 코드는 아래 링크를 참고 하세요.\n",
    "    - [Deploy PyTorch Models](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#deploy-pytorch-models)\n",
    "    - [디폴트 inference Code](https://github.com/aws/sagemaker-pytorch-inference-toolkit/blob/master/src/sagemaker_pytorch_serving_container/default_pytorch_inference_handler.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 로컬 모드 수행시, 새로운 로컬모드 수행을 위해서는 이전 사용했던 도커는 반드시 stop 해줘야 한다\n",
    "* docker ps -a 로 현재 수행중인 contatiner ID 확인 후\n",
    "* docker stop \"<<contatiner ID>>\"\n",
    "* docker container prune -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2.1.1 학습된 모델 경로 이동 (S3 -> Local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "artifact_path = pm.get_params(key=strPrefix + \"S3-MODEL-ARTIFACT\")\n",
    "model_data_dir = \"./model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-ap-northeast-2-419974056037/byom-model-output/pytorch-training-2023-02-28-04-38-34-563/model.tar.gz\n",
      "./model/\n",
      "download: s3://sagemaker-ap-northeast-2-419974056037/byom-model-output/pytorch-training-2023-02-28-04-38-34-563/model.tar.gz to model/model.tar.gz\n",
      "model.pth\n"
     ]
    }
   ],
   "source": [
    "%%sh -s {artifact_path} {model_data_dir}\n",
    "\n",
    "artifact_path=$1\n",
    "model_data_dir=$2\n",
    "\n",
    "echo $artifact_path\n",
    "echo $model_data_dir\n",
    "\n",
    "# 기존 데이터 삭제\n",
    "rm -rf $model_data_dir/*\n",
    "\n",
    "# 모델을 S3에서 로컬로 다운로드\n",
    "aws s3 cp $artifact_path $model_data_dir\n",
    "\n",
    "# 모델 다운로드 폴더로 이동\n",
    "cd $model_data_dir\n",
    "\n",
    "# 압축 해제\n",
    "tar -xvf model.tar.gz "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2.1.2. \"model_fn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from source.serve.inference import model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = model_fn(model_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2.1.2. \"input_fn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from source.serve.inference import input_fn\n",
    "from utils.cifar_utils import test_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: torch.Size([1, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "test_loader = test_data_loader()\n",
    "dataiter = iter(test_loader)\n",
    "images, _ = next(iter(dataiter))\n",
    "print (f\"Image size: {images.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = images.detach().cpu().numpy()\n",
    "dtype = data.dtype\n",
    "shape = data.shape\n",
    "payload={\"INPUT\": data.tolist(), \"SHAPE\":shape, \"DTYPE\": str(dtype)}\n",
    "payload_json = json.dumps(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.2392,  0.2471,  0.2941,  ...,  0.0745, -0.0118, -0.0902],\n",
       "          [ 0.1922,  0.1843,  0.2471,  ...,  0.0667, -0.0196, -0.0667],\n",
       "          [ 0.1843,  0.1843,  0.2392,  ...,  0.0902,  0.0196, -0.0588],\n",
       "          ...,\n",
       "          [-0.4667, -0.6706, -0.7569,  ..., -0.7020, -0.8980, -0.6863],\n",
       "          [-0.5216, -0.6157, -0.7255,  ..., -0.7961, -0.7725, -0.8431],\n",
       "          [-0.5765, -0.5608, -0.6471,  ..., -0.8118, -0.7333, -0.8353]],\n",
       "\n",
       "         [[-0.1216, -0.1294, -0.0902,  ..., -0.2549, -0.2863, -0.3333],\n",
       "          [-0.1216, -0.1373, -0.1059,  ..., -0.2549, -0.2863, -0.3098],\n",
       "          [-0.1373, -0.1451, -0.1294,  ..., -0.2314, -0.2549, -0.3020],\n",
       "          ...,\n",
       "          [-0.0275, -0.2157, -0.3098,  ..., -0.2392, -0.4980, -0.3333],\n",
       "          [-0.0902, -0.2000, -0.3333,  ..., -0.3569, -0.3569, -0.4980],\n",
       "          [-0.1608, -0.1765, -0.3020,  ..., -0.3961, -0.3412, -0.4745]],\n",
       "\n",
       "         [[-0.6157, -0.6314, -0.6000,  ..., -0.7176, -0.7176, -0.7412],\n",
       "          [-0.6000, -0.6863, -0.6471,  ..., -0.7569, -0.7490, -0.7333],\n",
       "          [-0.6314, -0.7412, -0.7176,  ..., -0.7333, -0.7333, -0.7412],\n",
       "          ...,\n",
       "          [ 0.3882,  0.1608,  0.0745,  ...,  0.1451, -0.1529, -0.0039],\n",
       "          [ 0.3176,  0.1608,  0.0353,  ...,  0.0196, -0.0118, -0.1608],\n",
       "          [ 0.2549,  0.1686,  0.0353,  ..., -0.0275,  0.0118, -0.1373]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = input_fn(request_body=payload_json, request_content_type=\"application/json\")\n",
    "input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2.1.3 predict_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from source.serve.inference import predict_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred = predict_fn(input_data, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2.1.4 output_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from source.serve.inference import output_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"pred\": [[0.20856785774230957, -1.7720112800598145, 0.8439523577690125, 1.9977813959121704, -0.2362309694290161, 0.8588781356811523, -0.2665247917175293, -1.3140302896499634, 1.8250188827514648, -1.5429975986480713]]}'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_fn(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 deploy (create an endpoint) in local env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* set instance types  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Feb 28 04:44:02 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   35C    P0    32W /  70W |   2073MiB / 15360MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      5275      C   ...vs/pytorch_p39/bin/python      941MiB |\n",
      "|    0   N/A  N/A     29279      C   ...3/envs/python3/bin/python      305MiB |\n",
      "|    0   N/A  N/A     32042      C   ...vs/pytorch_p39/bin/python      823MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "Instance type = local_gpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "try:\n",
    "    if subprocess.call(\"nvidia-smi\") == 0: instance_type = \"local_gpu\"\n",
    "    else:instance_type = \"local\"        \n",
    "except:pass\n",
    "print(\"Instance type = \" + instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "from sagemaker.serializers import CSVSerializer, NumpySerializer, JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer, NumpyDeserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint-name: endpoint-SM-BYOM-1677559467\n"
     ]
    }
   ],
   "source": [
    "strEndpointName = f\"endpoint-{strPrefix}{int(time.time())}\"\n",
    "print (f\"Endpoint-name: {strEndpointName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_esimator = PyTorchModel(\n",
    "    source_dir=\"./source/serve\",\n",
    "    entry_point=\"inference.py\",\n",
    "    model_data=pm.get_params(key=strPrefix + \"S3-MODEL-ARTIFACT\"),\n",
    "    role=pm.get_params(key=strPrefix + \"SAGEMAKER-ROLE-ARN\"),\n",
    "    framework_version='1.12.1',\n",
    "    py_version='py38',\n",
    "    model_server_workers=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "Attaching to 347j077gzz-algo-1-tdlqq\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Collecting sagemaker\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m   Downloading sagemaker-2.135.0.tar.gz (673 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m673.8/673.8 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m \u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m \u001b[?25hRequirement already satisfied: boto3 in /opt/conda/lib/python3.8/site-packages (from -r /opt/ml/model/code/requirements.txt (line 2)) (1.26.71)\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Requirement already satisfied: botocore in /opt/conda/lib/python3.8/site-packages (from -r /opt/ml/model/code/requirements.txt (line 3)) (1.29.71)\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from -r /opt/ml/model/code/requirements.txt (line 4)) (1.5.3)\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Collecting nvgpu\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m   Downloading nvgpu-0.9.0-py2.py3-none-any.whl (9.4 kB)\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Collecting attrs<23,>=20.3.0\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m   Downloading attrs-22.2.0-py3-none-any.whl (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m \u001b[?25hCollecting google-pasta\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m   Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m \u001b[?25hRequirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker->-r /opt/ml/model/code/requirements.txt (line 1)) (1.22.2)\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Collecting protobuf<4.0,>=3.1\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m   Downloading protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m \u001b[?25hCollecting protobuf3-to-dict<1.0,>=0.1.5\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m   Downloading protobuf3-to-dict-0.1.5.tar.gz (3.5 kB)\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m   Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m \u001b[?25hCollecting smdebug_rulesconfig==1.0.1\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m   Downloading smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl (20 kB)\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Collecting importlib-metadata<5.0,>=1.4.0\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m   Downloading importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker->-r /opt/ml/model/code/requirements.txt (line 1)) (23.0)\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Collecting pathos\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m   Downloading pathos-0.3.0-py3-none-any.whl (79 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m \u001b[?25hCollecting schema\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m   Downloading schema-0.7.5-py2.py3-none-any.whl (17 kB)\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from boto3->-r /opt/ml/model/code/requirements.txt (line 2)) (0.6.0)\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from boto3->-r /opt/ml/model/code/requirements.txt (line 2)) (1.0.1)\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.8/site-packages (from botocore->-r /opt/ml/model/code/requirements.txt (line 3)) (1.26.14)\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.8/site-packages (from botocore->-r /opt/ml/model/code/requirements.txt (line 3)) (2.8.2)\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->-r /opt/ml/model/code/requirements.txt (line 4)) (2022.7.1)\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Collecting termcolor\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m   Downloading termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Collecting flask-restful\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m   Downloading Flask_RESTful-0.3.9-py2.py3-none-any.whl (25 kB)\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from nvgpu->-r /opt/ml/model/code/requirements.txt (line 5)) (2.28.2)\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Collecting ansi2html\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m   Downloading ansi2html-1.8.0-py3-none-any.whl (16 kB)\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Collecting arrow\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m   Downloading arrow-1.2.3-py3-none-any.whl (66 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m \u001b[?25hCollecting flask\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m   Downloading Flask-2.2.3-py3-none-any.whl (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m \u001b[?25hCollecting tabulate\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m   Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Collecting pynvml\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m   Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m \u001b[?25hRequirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from nvgpu->-r /opt/ml/model/code/requirements.txt (line 5)) (5.9.4)\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from nvgpu->-r /opt/ml/model/code/requirements.txt (line 5)) (1.16.0)\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker->-r /opt/ml/model/code/requirements.txt (line 1)) (3.13.0)\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Collecting click>=8.0\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m   Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.6/96.6 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m \u001b[?25hCollecting Jinja2>=3.0\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m   Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m \u001b[?25hCollecting itsdangerous>=2.0\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m   Downloading itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Collecting Werkzeug>=2.2.2\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m   Downloading Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m \u001b[?25hCollecting aniso8601>=0.82\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m   Downloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m \u001b[?25hCollecting pox>=0.3.2\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m   Downloading pox-0.3.2-py3-none-any.whl (29 kB)\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Collecting ppft>=1.7.6.6\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m   Downloading ppft-1.7.6.6-py3-none-any.whl (52 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m \u001b[?25hCollecting dill>=0.3.6\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m   Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m \u001b[?25hCollecting multiprocess>=0.70.14\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m   Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m \u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->nvgpu->-r /opt/ml/model/code/requirements.txt (line 5)) (3.4)\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests->nvgpu->-r /opt/ml/model/code/requirements.txt (line 5)) (2.1.1)\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->nvgpu->-r /opt/ml/model/code/requirements.txt (line 5)) (2022.12.7)\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Collecting contextlib2>=0.5.5\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m   Downloading contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Collecting MarkupSafe>=2.0\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m   Downloading MarkupSafe-2.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Building wheels for collected packages: sagemaker, protobuf3-to-dict\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m   Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m \u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.135.0-py2.py3-none-any.whl size=911428 sha256=f2f0bec218d248cb2f4a876c3bfd4b2f27debb74a056a241a9b2c022f52f8ef3\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m   Stored in directory: /root/.cache/pip/wheels/ce/c2/3f/c92ead9fc44bb07546f1cba2aa9cb93d01b8f8e52a787e62ec\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m   Building wheel for protobuf3-to-dict (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m \u001b[?25h  Created wheel for protobuf3-to-dict: filename=protobuf3_to_dict-0.1.5-py3-none-any.whl size=4014 sha256=b5640a3fe3dd934c729ec85641e30b32ab64ac84e34009c4859c573613bb5176\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m   Stored in directory: /root/.cache/pip/wheels/fc/10/27/2d1e23d8b9a9013a83fbb418a0b17b1e6f81c8db8f53b53934\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Successfully built sagemaker protobuf3-to-dict\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Installing collected packages: aniso8601, termcolor, tabulate, smdebug_rulesconfig, pynvml, protobuf, ppft, pox, MarkupSafe, itsdangerous, importlib-metadata, google-pasta, dill, contextlib2, click, attrs, ansi2html, Werkzeug, schema, protobuf3-to-dict, multiprocess, Jinja2, arrow, pathos, flask, flask-restful, sagemaker, nvgpu\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Successfully installed Jinja2-3.1.2 MarkupSafe-2.1.2 Werkzeug-2.2.3 aniso8601-9.0.1 ansi2html-1.8.0 arrow-1.2.3 attrs-22.2.0 click-8.1.3 contextlib2-21.6.0 dill-0.3.6 flask-2.2.3 flask-restful-0.3.9 google-pasta-0.2.0 importlib-metadata-4.13.0 itsdangerous-2.1.2 multiprocess-0.70.14 nvgpu-0.9.0 pathos-0.3.0 pox-0.3.2 ppft-1.7.6.6 protobuf-3.20.3 protobuf3-to-dict-0.1.5 pynvml-11.5.0 sagemaker-2.135.0 schema-0.7.5 smdebug_rulesconfig-1.0.1 tabulate-0.9.0 termcolor-2.2.0\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m \u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m \u001b[0m\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m \u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m \u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m ['torchserve', '--start', '--model-store', '/.sagemaker/ts/models', '--ts-config', '/etc/sagemaker-ts.properties', '--log-config', '/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/etc/log4j2.xml', '--models', 'model=/opt/ml/model']\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Warning: Nashorn engine is planned to be removed from a future JDK release\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Warning: Nashorn engine is planned to be removed from a future JDK release\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m 2023-02-28T04:48:36,050 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m 2023-02-28T04:48:36,147 [INFO ] main org.pytorch.serve.ModelServer - \n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Torchserve version: 0.6.0\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m TS Home: /opt/conda/lib/python3.8/site-packages\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Current directory: /\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Temp directory: /home/model-server/tmp\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Number of GPUs: 1\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Number of CPUs: 8\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Max heap size: 7910 M\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Python executable: /opt/conda/bin/python3.8\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Config file: /etc/sagemaker-ts.properties\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Inference address: http://0.0.0.0:8080\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Management address: http://0.0.0.0:8080\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Metrics address: http://127.0.0.1:8082\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Model Store: /.sagemaker/ts/models\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Initial Models: model=/opt/ml/model\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Log dir: /logs\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Metrics dir: /logs\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Netty threads: 0\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Netty client threads: 0\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Default workers per model: 1\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Blacklist Regex: N/A\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Maximum Response Size: 6553500\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Maximum Request Size: 6553500\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Limit Maximum Image Pixels: true\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Prefer direct buffer: false\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Allowed Urls: [file://.*|http(s)?://.*]\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Custom python dependency for model allowed: false\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Metrics report format: prometheus\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Enable metrics API: true\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Workflow Store: /.sagemaker/ts/models\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Model config: N/A\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m 2023-02-28T04:48:36,154 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m 2023-02-28T04:48:36,173 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: /opt/ml/model\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m 2023-02-28T04:48:36,176 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m 2023-02-28T04:48:36,176 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m 2023-02-28T04:48:36,177 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m 2023-02-28T04:48:36,186 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m 2023-02-28T04:48:36,272 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m 2023-02-28T04:48:36,273 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m 2023-02-28T04:48:36,274 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m Model server started.\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m 2023-02-28T04:48:36,514 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m 2023-02-28T04:48:36,939 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m 2023-02-28T04:48:36,941 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]88\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m 2023-02-28T04:48:36,941 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m 2023-02-28T04:48:36,941 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m 2023-02-28T04:48:36,946 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m 2023-02-28T04:48:36,952 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m 2023-02-28T04:48:36,955 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1677559716955\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m 2023-02-28T04:48:36,987 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m 2023-02-28T04:48:37,058 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - ### model_fn ###\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m 2023-02-28T04:48:37,085 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m   File \"ts/metrics/metric_collector.py\", line 27, in <module>\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m     system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m   File \"/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py\", line 91, in collect_all\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m     value(num_of_gpu)\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m   File \"/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py\", line 72, in gpu_utilization\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m     statuses = list_gpus.device_statuses()\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m   File \"/opt/conda/lib/python3.8/site-packages/nvgpu/list_gpus.py\", line 67, in device_statuses\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m     return [device_status(device_index) for device_index in range(device_count)]\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m   File \"/opt/conda/lib/python3.8/site-packages/nvgpu/list_gpus.py\", line 67, in <listcomp>\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m     return [device_status(device_index) for device_index in range(device_count)]\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m   File \"/opt/conda/lib/python3.8/site-packages/nvgpu/list_gpus.py\", line 18, in device_status\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m     device_name = device_name.decode('UTF-8')\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m AttributeError: 'str' object has no attribute 'decode'\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m \n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m 2023-02-28T04:48:37,898 [INFO ] pool-2-thread-2 ACCESS_LOG - /172.18.0.1:33114 \"GET /ping HTTP/1.1\" 200 11\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m 2023-02-28T04:48:37,898 [INFO ] pool-2-thread-2 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:27b1f5865938,timestamp:1677559717\n",
      "!\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m 2023-02-28T04:48:38,993 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2006\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m 2023-02-28T04:48:38,994 [INFO ] W-9000-model_1.0 TS_METRICS - W-9000-model_1.0.ms:2812|#Level:Host|#hostname:27b1f5865938,timestamp:1677559718\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m 2023-02-28T04:48:38,995 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:34|#Level:Host|#hostname:27b1f5865938,timestamp:1677559718\n"
     ]
    }
   ],
   "source": [
    "local_predictor = local_esimator.deploy(\n",
    "    endpoint_name=strEndpointName,\n",
    "    instance_type=instance_type, \n",
    "    initial_instance_count=1,\n",
    "    serializer=JSONSerializer('application/json'), ## 미적용 시 default: application/x-npy, boto3 기반 invocation시 무시\n",
    "    deserializer=JSONDeserializer('application/json'), ## 미적용 시 default: application/x-npy, boto3 기반 invocation시 무시\n",
    "    wait=True,\n",
    "    log=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* inference (invocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from source.serve.inference import input_fn\n",
    "from utils.cifar_utils import test_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: torch.Size([1, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "test_loader = test_data_loader()\n",
    "dataiter = iter(test_loader)\n",
    "images, _ = next(iter(dataiter))\n",
    "print (f\"Image size: {images.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = images.detach().cpu().numpy()\n",
    "dtype = data.dtype\n",
    "shape = data.shape\n",
    "payload={\"INPUT\": data.tolist(), \"SHAPE\":shape, \"DTYPE\": str(dtype)}\n",
    "#payload_json = json.dumps(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m 2023-02-28T04:49:28,403 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1677559768403\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m 2023-02-28T04:49:28,405 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1677559768\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m 2023-02-28T04:49:28,406 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - ### input_fn ###\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m 2023-02-28T04:49:28,406 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - content_type: application/json\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m 2023-02-28T04:49:28,407 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - string\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m 2023-02-28T04:49:28,407 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - ### predict_fn ###\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m 2023-02-28T04:49:28,408 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - #### type of input data: <class 'torch.Tensor'>\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m 2023-02-28T04:49:29,951 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - ### output_fn ###\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m 2023-02-28T04:49:29,952 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:1545.46|#ModelName:model,Level:Model|#hostname:27b1f5865938,requestID:856cdd8d-f979-4e7d-b2a3-7115ea089e29,timestamp:1677559769\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m 2023-02-28T04:49:29,952 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1547\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m 2023-02-28T04:49:29,953 [INFO ] W-9000-model_1.0 ACCESS_LOG - /172.18.0.1:60812 \"POST /invocations HTTP/1.1\" 200 1558\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m 2023-02-28T04:49:29,953 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:27b1f5865938,timestamp:1677559717\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m 2023-02-28T04:49:29,954 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:27b1f5865938,timestamp:1677559769\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m 2023-02-28T04:49:29,954 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:27b1f5865938,timestamp:1677559769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pred': [[0.20856785774230957,\n",
       "   -1.7720112800598145,\n",
       "   0.8439523577690125,\n",
       "   1.9977813959121704,\n",
       "   -0.2362309694290161,\n",
       "   0.8588781356811523,\n",
       "   -0.2665247917175293,\n",
       "   -1.3140302896499634,\n",
       "   1.8250188827514648,\n",
       "   -1.5429975986480713]]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m 2023-02-28T04:49:36,996 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m   File \"ts/metrics/metric_collector.py\", line 27, in <module>\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m     system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m   File \"/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py\", line 91, in collect_all\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m     value(num_of_gpu)\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m   File \"/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py\", line 72, in gpu_utilization\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m     statuses = list_gpus.device_statuses()\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m   File \"/opt/conda/lib/python3.8/site-packages/nvgpu/list_gpus.py\", line 67, in device_statuses\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m     return [device_status(device_index) for device_index in range(device_count)]\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m   File \"/opt/conda/lib/python3.8/site-packages/nvgpu/list_gpus.py\", line 67, in <listcomp>\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m     return [device_status(device_index) for device_index in range(device_count)]\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m   File \"/opt/conda/lib/python3.8/site-packages/nvgpu/list_gpus.py\", line 18, in device_status\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m     device_name = device_name.decode('UTF-8')\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m AttributeError: 'str' object has no attribute 'decode'\n",
      "\u001b[36m347j077gzz-algo-1-tdlqq |\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/sagemaker/local/image.py\", line 854, in run\n",
      "    _stream_output(self.process)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/sagemaker/local/image.py\", line 916, in _stream_output\n",
      "    raise RuntimeError(\"Process exited with code: %s\" % exit_code)\n",
      "RuntimeError: Process exited with code: 137\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/sagemaker/local/image.py\", line 859, in run\n",
      "    raise RuntimeError(msg)\n",
      "RuntimeError: Failed to run: ['docker-compose', '-f', '/tmp/tmpbg4143ka/docker-compose.yaml', 'up', '--build', '--abort-on-container-exit'], Process exited with code: 137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m347j077gzz-algo-1-tdlqq exited with code 137\n",
      "\u001b[0mAborting on container exit...\n"
     ]
    }
   ],
   "source": [
    "pred_results = local_predictor.predict(payload)\n",
    "pred_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 deploy (create an endpoint) in cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_type = \"ml.g4dn.xlarge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint-name: endpoint-SM-BYOM-1677559877\n"
     ]
    }
   ],
   "source": [
    "strEndpointName = f\"endpoint-{strPrefix}{int(time.time())}\"\n",
    "print (f\"Endpoint-name: {strEndpointName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cloud_esimator = PyTorchModel(\n",
    "    source_dir=\"./source/serve\",\n",
    "    entry_point=\"inference.py\",\n",
    "    model_data=pm.get_params(key=strPrefix + \"S3-MODEL-ARTIFACT\"),\n",
    "    role=pm.get_params(key=strPrefix + \"SAGEMAKER-ROLE-ARN\"),\n",
    "    framework_version='1.12.1',\n",
    "    py_version='py38',\n",
    "    model_server_workers=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "cloud_predictor = cloud_esimator.deploy(\n",
    "    endpoint_name=strEndpointName,\n",
    "    instance_type=instance_type, \n",
    "    initial_instance_count=1,\n",
    "    serializer=JSONSerializer('application/json'),\n",
    "    deserializer=JSONDeserializer('application/json'),\n",
    "    wait=True,\n",
    "    log=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Store suceess'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pm.put_params(key=strPrefix + \"ENDPOINT-NAME\", value=strEndpointName, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2.3.1 Inference by SageMaker SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: torch.Size([1, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "test_loader = test_data_loader()\n",
    "dataiter = iter(test_loader)\n",
    "images, _ = next(iter(dataiter))\n",
    "print (f\"Image size: {images.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = images.detach().cpu().numpy()\n",
    "dtype = data.dtype\n",
    "shape = data.shape\n",
    "payload={\"INPUT\": data.tolist(), \"SHAPE\":shape, \"DTYPE\": str(dtype)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.9 ms, sys: 2.66 ms, total: 16.6 ms\n",
      "Wall time: 1.86 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pred': [[0.20856785774230957,\n",
       "   -1.7720112800598145,\n",
       "   0.8439523577690125,\n",
       "   1.9977813959121704,\n",
       "   -0.2362309694290161,\n",
       "   0.8588781356811523,\n",
       "   -0.2665247917175293,\n",
       "   -1.3140302896499634,\n",
       "   1.8250188827514648,\n",
       "   -1.5429975986480713]]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "pred_results = cloud_predictor.predict(payload)\n",
    "pred_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2.3.2 inference by boto3\n",
    "    - serialization을 customization 할 수 있음 (아래 pickle based serialization 참조)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### boto3 기반 invocation시 runtime_client가 필요함\n",
    "* 이때 엔드포인트의 위치 (local/cloud)를 반드시 구분해 줘야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<botocore.client.SageMakerRuntime at 0x7fef469c6670>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if instance_type == 'local_gpu': runtime_client = sagemaker.local.LocalSagemakerRuntimeClient()    \n",
    "else: runtime_client = boto3.Session().client('sagemaker-runtime')\n",
    "runtime_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def invoke_endpoint(runtime_client, endpoint_name, payload, content_type):\n",
    "    response = runtime_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, \n",
    "        ContentType=content_type, \n",
    "        Body=payload,\n",
    "    )\n",
    "\n",
    "    result = response['Body'].read().decode().splitlines()    \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* json based serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [중요] boto3로 inference 할 때는 payload를 반드시 serialization 해서 넣어줘야 함\n",
    "* sagemaker SDK는 deploy시 셋팅한 방식대로 serialization을 자동으로 해 준다\n",
    "* boto3는 사용자가 직접 serialization을 한 후 inference 한다. (대신 serialization 자유도가 있다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload_dump = json.dumps(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.12499523162841797 seconds ---\n",
      "result:  ['{\"pred\": [[0.20856785774230957, -1.7720112800598145, 0.8439523577690125, 1.9977813959121704, -0.2362309694290161, 0.8588781356811523, -0.2665247917175293, -1.3140302896499634, 1.8250188827514648, -1.5429975986480713]]}']\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "result = invoke_endpoint(\n",
    "    runtime_client=runtime_client,\n",
    "    endpoint_name=pm.get_params(key=strPrefix + \"ENDPOINT-NAME\"), \n",
    "    payload=payload_dump,\n",
    "    content_type='application/json'\n",
    ")\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print('result: ', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pickle based serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: torch.Size([1, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "test_loader = test_data_loader()\n",
    "dataiter = iter(test_loader)\n",
    "images, _ = next(iter(dataiter))\n",
    "print (f\"Image size: {images.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pickle 기반 serialization시 numpy, tensor등을 별도 변환없이 그대로 전송할 수 있음\n",
    "* 아래 INPUT의 경우 json으로 serialization 할 때 처럼 .tolist()를 할 필요 없음\n",
    "* pickle vs json: https://www.educba.com/python-pickle-vs-json/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = images.detach().cpu().numpy()\n",
    "dtype = data.dtype\n",
    "shape = data.shape\n",
    "payload={\"INPUT\": data, \"SHAPE\":shape, \"DTYPE\": str(dtype)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload_dump = pickle.dumps(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/sagemaker-train-deploy/utils/ssm.py\u001b[0m in \u001b[0;36mget_params\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     25\u001b[0m         )\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Parameters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_all_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "result = invoke_endpoint(\n",
    "    runtime_client=runtime_client,\n",
    "    endpoint_name=pm.get_params(key=strPrefix + \"ENDPOINT-NAME\"), \n",
    "    payload=payload_dump,\n",
    "    content_type='application/pickle'\n",
    ")\n",
    "\n",
    "deserialized_results = json.loads(result[0])\n",
    "print('result: ', deserialized_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result:  {'pred': [[0.20856785774230957, -1.7720112800598145, 0.8439523577690125, 1.9977813959121704, -0.2362309694290161, 0.8588781356811523, -0.2665247917175293, -1.3140302896499634, 1.8250188827514648, -1.5429975986480713]]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "deserialized_results = json.loads(result[0])\n",
    "print('result: ', deserialized_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "1f4a227f2552de404565d6626d9115986c409361ad6ef0bf195ad88ccd012b73"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
